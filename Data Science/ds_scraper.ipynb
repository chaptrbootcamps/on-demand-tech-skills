{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Science Jobs Scraper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "source": [
    "### Target Skills"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Skills:  50  technologies\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Skill     Category\n",
       "0      python  Programming\n",
       "1           r  Programming\n",
       "2         sql     Database\n",
       "3        java  Programming\n",
       "4  javascript  Programming"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Skill</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>python</td>\n      <td>Programming</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>r</td>\n      <td>Programming</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>sql</td>\n      <td>Database</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>java</td>\n      <td>Programming</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>javascript</td>\n      <td>Programming</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "skills_df = pd.read_csv(\"ds_skills.csv\")\n",
    "skills_df[\"Skill\"] = skills_df[\"Skill\"].str.lower()\n",
    "print(\"Total Skills: \", skills_df.shape[0], \" technologies\")\n",
    "\n",
    "skills_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['python', 'r', 'sql', 'java', 'javascript', 'c', 'scala', 'julia', 'sas', 'matlab', 'matplotlib', 'seaborn', 'bokeh', 'plotly', 'ggplot', 'numpy', 'scipy', 'pandas', 'scikit', 'keras', 'pytorch', 'xgboost', 'tensorflow', 'aws', 'azure', 'haskel', 'swift', 'octave', 'perl', 'lisp', 'excel', 'power', 'tableau', 'google', 'spark', 'hadoop', 'git', 'spss', 'stata', 'pig', 'hbase', 'docker', 'kubernetes', 'statsmodels', 'nltk', 'scrapy', 'beautiful', 'selenium', 'github', 'nosql', 'data']\n"
     ]
    }
   ],
   "source": [
    "skills = skills_df['Skill'].to_list()\n",
    "skills.append(\"data\")\n",
    "\n",
    "print(skills)"
   ]
  },
  {
   "source": [
    "### HTML Content Extractor (Function)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsoup(url):\n",
    "    try: \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    except:\n",
    "        print(response.status_code)"
   ]
  },
  {
   "source": [
    "## Glassdoor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<module 'requests.status_codes' from '/Users/cyrilmichino/opt/anaconda3/lib/python3.7/site-packages/requests/status_codes.py'>\n"
     ]
    }
   ],
   "source": [
    "soup = getsoup(\"https://www.indeed.com/\")"
   ]
  },
  {
   "source": [
    "## Indeed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['3,676',\n",
       " '2,481',\n",
       " '2,666',\n",
       " '1,095',\n",
       " '278',\n",
       " '440',\n",
       " '643',\n",
       " '62',\n",
       " '727',\n",
       " '445',\n",
       " '130',\n",
       " '40',\n",
       " '83',\n",
       " '56',\n",
       " '84',\n",
       " '307',\n",
       " '155',\n",
       " '361',\n",
       " '423',\n",
       " '269',\n",
       " '403',\n",
       " '64',\n",
       " '674',\n",
       " '1,129',\n",
       " '511',\n",
       " '1',\n",
       " '10',\n",
       " '40',\n",
       " '107',\n",
       " '2',\n",
       " '508',\n",
       " '241',\n",
       " '1,088',\n",
       " '179',\n",
       " '1,267',\n",
       " '911',\n",
       " '261',\n",
       " '162',\n",
       " '61',\n",
       " '148',\n",
       " '122',\n",
       " '278',\n",
       " '203',\n",
       " '35',\n",
       " '41',\n",
       " '2',\n",
       " '1',\n",
       " '7',\n",
       " '172',\n",
       " '336']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "indeed_list = []\n",
    "\n",
    "for term in skills:\n",
    "    url = f'https://www.indeed.com/jobs?q=%22data+scientist%22+%22{term}%22&l=United+States'\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        count_str = soup.find('div', id=\"searchCountPages\").get_text().lower()\n",
    "        numb = count_str.split()\n",
    "        indeed_list.append(numb[-2])\n",
    "    except Exception as e:\n",
    "        print(f'error: {e}')\n",
    "        \n",
    "indeed_list"
   ]
  },
  {
   "source": [
    "## Monster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}